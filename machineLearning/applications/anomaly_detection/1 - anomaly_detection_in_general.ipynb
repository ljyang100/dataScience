{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://en.wikipedia.org/wiki/Anomaly_detection\n",
    "\n",
    "* Typically the anomalous items will translate to some kind of problem such as bank fraud, a structural defect, medical problems or errors in a text. Anomalies are also referred to as outliers, novelties, noise, deviations and exceptions. \n",
    "\n",
    "* As shown in the link , there are many ways for anomaly detection. These include three broad categories: supervised, unsupervised and semi-supervised. All three types of methods will be studied in terms of when and how they should be applied. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with unsupervised learning -- detecting outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Unsupervised learning is usually used in one of major types of anomaly detection, i.e., outlier detection.\n",
    "\n",
    "* The key assumption for most outlier detection algorithms is that the positive class to be detected is indeed appearing as outliers in a distribution relevant to the linearly or nonlinearly mapped space. If this assumption is not valid for some cases, then this family of algorithms will not work. \n",
    "\n",
    "* Different outliers detection algorithms listed in https://towardsdatascience.com/anomaly-detection-with-pyod-b523fc47db9 amounts to give different distributions in different bases. If positive class to be detected happened to be outliers in one of the bases, then the corresponding algorithm will work.  \n",
    "\n",
    "* Outliers are supposed to eliminated in usual supervised learning, particularly in regression models due to its influences on the model accuracy. However, here the outliers are the signal to be found. \n",
    "\n",
    "* In anomaly detection with unsupervised learning, concepts such as train-test splitting and training/fitting are also involved, even though they are only involved supervised learning in most cases. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive pictures of outlier detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The simplest picture is a 2D data cloud satisfying normal distribution, where we can easily define the outliers. For high-dimensional data, we can also construct high-dimensional normal distribution and calculate the outliers directly from the original vector space of the data. \n",
    "\n",
    "* PCA picture. Image a 2D cloud of data, and along the principal axis there are most of the data. Then the data far away from the axis is considered to be outliers or an anomaly. This picture is useful when we have linear relation within the data. Here we assume the major data cloud sits in a subspace of the original high-dimensional vector space. Calculating distance (errors) under the subspace may mitigate the so-called curse of dimensionality.  FURTHERMORE, WE CAN REDUCE THE VARIANCE CAUSED BY FEATURE CORRELATIONS. \n",
    "\n",
    "* Auto-encoder. Unlike PCA, auto-encoders can come up with a reduced nonlinear-mapped space. This is better than PCA as it can handle both linear and nonlinear data relations. Here we can use reconstruction error to determine whether a sample is outlier or not. The key advantages of using autoencoders to detect outliers lie in two factors: handling nonlinearity among data, and reduce the dimension due to strong correlation (might be nonlinear correlation) among samples. If the data sample dimension is low, or if there are no strong correlations among samples, or there is only linear relationship among samples, then auto-encoders are not necessary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers detection with unsupervised learning -- KNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/anomaly-detection-with-pyod-b523fc47db9. Outliers detection with KNN by calculating outliers score, essentially distance scores. \n",
    "\n",
    "* Although unsupervised techniques are powerful in detecting outliers, they are prone to overfitting and unstable results. The solution is to train multiple models then aggregate the scores.\n",
    "\n",
    "* Although a unsupervised learning (with the usual supervised learning KNN), we have train-split in anomaly detection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier dection with unsupervised learning -- autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.pyimagesearch.com/2020/03/02/anomaly-detection-with-keras-tensorflow-and-deep-learning/\n",
    "\n",
    "* See the pyimagesearch example under the /anomaly_fraud_detection. All data including the majority of negative class and the minority of positive class samples are entered into the training process. Labels are ignored and thus we have a pure unsupervised outliers detection. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection with autoencoders -- simulation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/anomaly-detection-with-autoencoder-b4cdce4866a6. Also see downloaded version.\n",
    "\n",
    "* This is high-level auto-encoders in PyOD package with the format as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf1 = AutoEncoder(hidden_neurons =[25, 2, 2, 25])\n",
    "clf1.fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anomaly detection with autoencoders -- with Kaggle data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Be familiar with the code in the relevant folder. \n",
    "* Here only the normal data is used to train the autoencoder. However, the author says this is an unsupervised or semisupervised algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolation forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://www.pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/\n",
    "\n",
    "* Here again we use only normal samples to train the data. \n",
    "\n",
    "* Isolation forests are a type of ensemble algorithm and consist of multiple decision trees used to partition the input dataset into distinct groups of inliers.\n",
    "\n",
    "* Isolation Forests accept an input dataset (white points) and then build a manifold surrounding them.\n",
    "\n",
    "* At test time, the Isolation Forest can then determine if the input points fall inside the manifold (standard events; green points) or outside the high-density area (anomaly events; red points).\n",
    "\n",
    "* Isolation forests is also an unsupervised algorithm. Because we use only the normal data to train the model, some people call this a supervised model. See below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One way of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.pyimagesearch.com/2020/01/20/intro-to-anomaly-detection-with-opencv-computer-vision-and-scikit-learn/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be broken down into two subclasses. According to the following classification, the kaggle dataset and isolation forests example should belong to the supervised learing. However, generally, they should belong to unsupervised or semi-supervised learning. \n",
    "\n",
    "* Outlier detection: Our input dataset contains examples of both standard events and anomaly events. These algorithms seek to fit regions of the training data where the standard events are most concentrated, disregarding, and therefore isolating, the anomaly events. Such algorithms are often trained in an unsupervised fashion (i.e., without labels). We sometimes use these methods to help clean and pre-process datasets before applying additional machine learning techniques.\n",
    "\n",
    "* Novelty detection: Unlike outlier detection, which includes examples of both standard and anomaly events, novelty detection algorithms have only the standard event data points (i.e., no anomaly events) during training time. During training, we provide these algorithms with labeled examples of standard events (supervised learning). At testing/prediction time novelty detection algorithms must detect when an input data point is an outlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection with supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This include all other what I did before on the supervised learning algorithms with imbalanced dataset. \n",
    "* Check all other notes for various ways such as using class_weights, over/under sampling, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One way of classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some side notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Comments on the noise: The noise here, e.g. in terms of images, is an exceptional image that is different from all other common images. When we talking about the denoising of a picture with auto-encoder, we are talking about the noise within a single picture. Some essential and repetitive features will be \"recorded\" and noise will not. In other words, noise within a picture cannot go through the autoencoder, right? \n",
    "\n",
    "* Comments on the correlation: The proper scenario for using auto-encoders is that there are strong correlations among different samples. For example we have so many digit pictures, then there will be strong correlation among pictures will number '1'. In this case, we could have a big ratio on dimension reduction. Be careful the correlation is not all the correlations of pixels on a single image. Convolutional neural network can handle such correlations among samples. If there are also sequence or ordering effect among samples, then we may also need combine conv2D to LSTM in auto-encoders. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
