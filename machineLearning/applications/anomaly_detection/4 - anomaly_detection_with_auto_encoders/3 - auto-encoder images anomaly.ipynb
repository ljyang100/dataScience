{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://towardsdatascience.com/auto-encoder-what-is-it-and-what-is-it-used-for-part-1-3e5c6f017726"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Autoencoder is an unsupervised artificial neural network that learns how to efficiently compress and encode data then learns how to reconstruct the data back from the reduced encoded representation to a representation that is as close to the original input as possible.\n",
    "\n",
    "* Autoencoder, by design, reduces data dimensions by learning how to ignore the noise in the data.\n",
    "\n",
    "* The network architecture for autoencoders can vary between a simple FeedForward network, LSTM network or Convolutional Neural Network depending on the use case. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder for Anomaly Detection:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you have correlated input data, the autoencoder method will work very well because the encoding operation relies on the correlated features to compress the data. \n",
    "\n",
    "* It seems correlation is often the major reason for compression. Imagine the picture of PCA. \n",
    "\n",
    "* Let's train an autoencoder on the MNIST dataset using a simple FeedForward neural network. We can achieve this by building a simple 6 layers network as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 1s 0us/step\n",
      "WARNING:tensorflow:From C:\\Users\\ljyan\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\Users\\ljyan\\anaconda3\\envs\\tensorflow-gpu\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 28us/step - loss: 0.0745 - val_loss: 0.0501\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0416 - val_loss: 0.0347\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0314 - val_loss: 0.0280\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0270 - val_loss: 0.0253\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0249 - val_loss: 0.0237\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0234 - val_loss: 0.0223\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0222 - val_loss: 0.0212\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0212 - val_loss: 0.0203\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 17us/step - loss: 0.0203 - val_loss: 0.0196\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 1s 18us/step - loss: 0.0196 - val_loss: 0.0190\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras import optimizers\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "train_x = x_train.reshape(60000, 784) / 255\n",
    "val_x = x_test.reshape(10000, 784) / 255\n",
    "\n",
    "autoencoder = Sequential()\n",
    "autoencoder.add(Dense(512,  activation='elu', input_shape=(784,)))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(10,    activation='linear', name=\"bottleneck\"))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(512,  activation='elu'))\n",
    "autoencoder.add(Dense(784,  activation='sigmoid'))\n",
    "autoencoder.compile(loss='mean_squared_error', optimizer = Adam())\n",
    "trained_model = autoencoder.fit(train_x, train_x, batch_size=1024, epochs=10, verbose=1, validation_data=(val_x, val_x))\n",
    "encoder = Model(autoencoder.input, autoencoder.get_layer('bottleneck').output)\n",
    "encoded_data = encoder.predict(train_x)  # bottleneck representation\n",
    "decoded_output = autoencoder.predict(train_x)        # reconstruction\n",
    "encoding_dim = 10\n",
    "\n",
    "# return the decoder\n",
    "encoded_input = Input(shape=(encoding_dim,))\n",
    "decoder = autoencoder.layers[-3](encoded_input)\n",
    "decoder = autoencoder.layers[-2](decoder)\n",
    "decoder = autoencoder.layers[-1](decoder)\n",
    "decoder = Model(encoded_input, decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* As you can see in the output, the last reconstruction loss/error for the validation set is 0.0193 which is great. Now, if I pass any normal image from the MNIST dataset, the reconstruction loss will be very low (< 0.02) BUT if I tried to pass any other different image (outlier or anomaly), we will get a high reconstruction loss value because the network failed to reconstruct the image/input that is considered an anomaly.\n",
    "\n",
    "* Notice in the code above, you can use only the encoder part to compress some data or images and you can also only use the decoder part to decompress the data by loading the decoder layers.\n",
    "\n",
    "* Let’s do some anomaly detection. The code below uses two different images to predict the anomaly score (reconstruction error) using the autoencoder network we trained above. the first image is from the MNIST and the result is 5.43209. This means that the image is not an anomaly. The second image I used, is a completely random image that doesn’t belong to the training dataset and the results were: 6789.4907. This high error means that the image is an anomaly. The same concept applies to any type of dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from keras.preprocessing import image\n",
    "# if the img.png is not one of the MNIST dataset that the model was trained on, the error will be very high.\n",
    "img = image.load_img(\"./img.png\", target_size=(28, 28), color_mode = \"grayscale\")\n",
    "input_img = image.img_to_array(img)\n",
    "inputs = input_img.reshape(1,784)\n",
    "target_data = autoencoder.predict(inputs)\n",
    "dist = np.linalg.norm(inputs - target_data, axis=-1)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Denoising:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Denoising or noise reduction is the process of removing noise from a signal. This can be an image, audio or a document. You can train an Autoencoder network to learn how to remove noise from pictures. In order to try out this use case, let’s re-use the famous MNIST dataset and let’s create some synthetic noise in the dataset. The code below will simply add some noise to the dataset then plot a few pictures to make sure that we’ve successfully created them.\n",
    "# The code below is from the Keras Blogs\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "#Print one image to see the noise\n",
    "plt.imshow(x_test_noisy[1].reshape(28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1f3ec955248>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# The code below is from the Keras Blogs\n",
    "# https://blog.keras.io/building-autoencoders-in-keras.html\n",
    "import matplotlib.pyplot as plt\n",
    "noise_factor = 0.5\n",
    "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape) \n",
    "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape) \n",
    "\n",
    "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
    "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
    "\n",
    "#Print one image to see the noise\n",
    "plt.imshow(x_test_noisy[1].reshape(28, 28))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the code above is the image below, which is pretty noisy and fuzzy:\n",
    "\n",
    "In this example, let’s build a Convolutional Autoencoder Neural Network. I will walk through each line of building the network:\n",
    "First, we define the input layer and the dimensions of the input data. MNIST dataset has images that are reshaped to be 28 X 28 in dimensions. Since the images are greyscaled, the colour channel of the image will be 1 so the shape is (28, 28, 1).\n",
    "The second layer is the convolution layer, this layer creates a convolution kernel that is convolved with the layer input to produce a tensor of outputs. 32 is the number of output filters in the convolution and (3, 3) is the kernel size.\n",
    "After each convolution layer, we use MaxPooling function to reduce the dimensions. The (28, 28, 32) is reduced by a factor of two so it will be (14, 14, 32) after the first MaxPooling then (7, 7, 32) after the second MaxPooling. This is the encoded representation of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Sequential()\n",
    "autoencoder.add(Dense(512,  activation='elu', input_shape=(784,)))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(10,    activation='linear', name=\"bottleneck\"))\n",
    "autoencoder.add(Dense(128,  activation='elu'))\n",
    "autoencoder.add(Dense(512,  activation='elu'))\n",
    "autoencoder.add(Dense(784,  activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))\n",
    "\n",
    "nn = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "nn = MaxPooling2D((2, 2), padding='same')(nn)\n",
    "nn = Conv2D(32, (3, 3), activation='relu', padding='same')(nn)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below is the reconstruction part of the original digits. This is where the network actually learns how to remove the noise from the input images. We use UpSampling function to rebuild the images to the original dimensions (28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Conv2D' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-16c75ae02f84>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mUpSampling2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdecoded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'sigmoid'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'same'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Conv2D' is not defined"
     ]
    }
   ],
   "source": [
    "nn = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded)\n",
    "nn = UpSampling2D((2, 2))(nn)\n",
    "nn = Conv2D(32, (3, 3), activation='relu', padding='same')(nn)\n",
    "nn = UpSampling2D((2, 2))(nn)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the last remaining step is to create the model, compile it then start the training. We do this by running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta',loss='binary_crossentropy')\n",
    "autoencoder.fit(x_train_noisy, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                validation_data=(x_test_noisy, x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the training is complete, I try to pass one noisy image through the network and the results are quite impressive, the noise was completely removed:\n",
    "\n",
    "If you scale the ConvNet above, you can use it to denoise any type of images, audio or scanned documents.\n",
    "In this part of the article, I covered two important use cases for autoencoders and I build two different neural network architectures — CNN and FeedForward. In part 2, I will cover another 2 important use cases for Autoencoders. The first one will be how to use autoencoder with a sequence of data by building an LSTM network and the second use case is a called Variational Autoencoder (VAE) which is mainly used in Generative Models and generating data or images. Stay tuned!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder with sequence data - LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Autoencoder (VAE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
